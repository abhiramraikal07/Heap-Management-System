{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiramraikal07/Heap-Management-System/blob/main/NFT_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7QKY5REptkP"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "#Dog Breed Identification(NFT Project)\n",
        "\n",
        "---\n",
        "\n",
        "* [Step 1](#step1): Import Datasets\n",
        "* [Step 2](#step2): Detect Dogs\n",
        "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
        "* [Step 4](#step4): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
        "* [Step 5](#step5): Write your Algorithm\n",
        "* [Step 6](#step6): Test Your Algorithm\n",
        "\n",
        "---\n",
        "<a id='step0'></a>\n",
        "## Step 1: Import Datasets\n",
        "\n",
        "* Download the [dog dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip).  Unzip the folder and place it in this project's home directory, at the location `/dog_images`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5tybxxcp2IK",
        "outputId": "c490d870-cb96-4666-e8e3-a363f46dc97d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%time\n",
        "# https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
        "# https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\n",
        "\n",
        "import requests, zipfile, io, os, shutil\n",
        "\n",
        "root = '/content'\n",
        "dogimages_url = \"https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\"\n",
        "humanimages_url = \"https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\"\n",
        "\n",
        "data_path = os.path.join(root,'data')\n",
        "dogimages_path = os.path.join(data_path,'dogImages')\n",
        "humanimages_path = os.path.join(data_path,'lfw')\n",
        "data_path = os.path.join(root,'data')\n",
        "if not os.path.exists(data_path): os.mkdir(data_path)\n",
        "\n",
        "# if os.path.exists(dogimages_path):shutil.rmtree(dogimages_path)\n",
        "# if os.path.exists(humanimages_path):shutil.rmtree(humanimages_path)\n",
        "\n",
        "if not os.path.exists(dogimages_path):\n",
        "  print(\"downloading dog images\")\n",
        "  r = requests.get(dogimages_url) \n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content)) \n",
        "  z.extractall(data_path)\n",
        "\n",
        "if not os.path.exists(humanimages_path):\n",
        "  print(\"downloading human images\")\n",
        "  r = requests.get(humanimages_url) \n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content)) \n",
        "  z.extractall(data_path)\n",
        "\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "# load filenames for human and dog images\n",
        "human_files = np.array(glob(\"/content/data/lfw/*/*\"))\n",
        "dog_files = np.array(glob(\"/content/data/dogImages/*/*/*\"))\n",
        "\n",
        "# print number of images in each dataset\n",
        "print('There are %d total human images.' % len(human_files))\n",
        "print('There are %d total dog images.' % len(dog_files))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 13233 total human images.\n",
            "There are 8351 total dog images.\n",
            "CPU times: user 114 ms, sys: 67.1 ms, total: 181 ms\n",
            "Wall time: 183 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD_0RvqpMVTU",
        "outputId": "f5174a71-cdc8-4436-d88d-419dffe0f94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "if not os.path.exists('/content/haarcascades'): os.mkdir('/content/haarcascades')\n",
        "haarcascades_url = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml'\n",
        "!wget -O /content/haarcascades/haarcascade_frontalface_alt.xml {haarcascades_url}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-25 23:23:38--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 676709 (661K) [text/plain]\n",
            "Saving to: ‘/content/haarcascades/haarcascade_frontalface_alt.xml’\n",
            "\n",
            "\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/haarcascad 100%[===================>] 660.85K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-12-25 23:23:39 (14.8 MB/s) - ‘/content/haarcascades/haarcascade_frontalface_alt.xml’ saved [676709/676709]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdmarNb_ptpJ"
      },
      "source": [
        "---\n",
        "<a id='step2'></a>\n",
        "## Step 2: Detect Dogs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmmsm0jyptpN",
        "outputId": "69b5a5d6-e389-48d8-ff88-cc6946288b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# define VGG16 model\n",
        "VGG16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# move model to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    VGG16 = VGG16.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:11<00:00, 49.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HpPXKBzptp3"
      },
      "source": [
        "### (IMPLEMENTATION) Making Predictions with a Pre-trained Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWlgf97Nptp7",
        "outputId": "850f4648-8f84-46eb-d579-f6739ff4c3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import os\n",
        "\n",
        "def VGG16_predict(img_path):\n",
        "    '''\n",
        "    Use pre-trained VGG-16 model to obtain index corresponding to \n",
        "    predicted ImageNet class for image at specified path\n",
        "    \n",
        "    Args:\n",
        "        img_path: path to an image\n",
        "        \n",
        "    Returns:\n",
        "        Index corresponding to VGG-16 model's prediction\n",
        "    '''\n",
        "    \n",
        "    ## TODO: Complete the function.\n",
        "    ## Load and pre-process an image from the given img_path\n",
        "    ## Return the *index* of the predicted class for that image\n",
        "    \n",
        "    # data transformation\n",
        "    batch_size = 64\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    image_transforms = transforms.Compose([\n",
        "                            transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "                            transforms.RandomRotation(degrees=15),\n",
        "                            transforms.ColorJitter(),\n",
        "                            transforms.RandomHorizontalFlip(),\n",
        "                            transforms.CenterCrop(size=224),  # Image net standards\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                 [0.229, 0.224, 0.225])  # Imagenet standards\n",
        "                        ])\n",
        "    \n",
        "    image = image_transforms(img)[:3,:,:].unsqueeze(0)\n",
        "#     image = image_transforms(img)\n",
        "#     print(image_transformation)\n",
        "    \n",
        "    if use_cuda:\n",
        "        image = image.cuda()\n",
        "    output = VGG16(image)\n",
        "\n",
        "    _,pred = torch.max(output, dim=1)\n",
        "    pred=pred.cpu()\n",
        "    pred = pred.data.numpy()[0]\n",
        "        \n",
        "    return pred # predicted class index\n",
        "\n",
        "VGG16_predict(dog_files[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwmVxvlptqS"
      },
      "source": [
        "### (IMPLEMENTATION) Write a Dog Detector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho5sYtCwptqb",
        "outputId": "43c0215d-060e-44ea-eaee-1c12eec1a720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "### returns \"True\" if a dog is detected in the image stored at img_path\n",
        "def dog_detector(img_path):\n",
        "    ## TODO: Complete the function.\n",
        "#     in VGG16 index 151 to 268 are dog classifications\n",
        "    \n",
        "    return VGG16_predict(img_path)>= 151 and VGG16_predict(img_path)<=268 # true/false\n",
        "print(dog_detector(human_files[3]))\n",
        "print(dog_detector(dog_files[7]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVSq-5gaptqt"
      },
      "source": [
        "### (IMPLEMENTATION) Assess the Dog Detector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhMfc2i3ptrM",
        "outputId": "2e09f989-d0a4-4903-fbd2-173fd906238e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "### TODO: Test the performance of the dog_detector function\n",
        "### on the images in human_files_short and dog_files_short. \n",
        "\n",
        "human_files_count=0\n",
        "dog_files_count=0\n",
        "for i in range(len(human_files_short)):\n",
        "    if dog_detector(human_files_short[i]):\n",
        "        human_files_count+=1\n",
        "    if dog_detector(dog_files_short[i]):\n",
        "        dog_files_count+=1\n",
        "print(\"number of dog faces detected in human_fiiles_short {0}\".format(human_files_count))\n",
        "print(\"number of dog faces detected in dog_files_short {0}\".format(dog_files_count))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of dog faces detected in human_fiiles_short 5\n",
            "number of dog faces detected in dog_files_short 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MympVvOnptud"
      },
      "source": [
        "---\n",
        "<a id='step3'></a>\n",
        "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
        "\n",
        "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2CC9eL7ptug",
        "outputId": "5a6d00ed-67b7-4c70-d861-7ac6233dcf15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_path = '/content/data/dogImages/train/'\n",
        "val_path = '/content/data/dogImages/valid'\n",
        "test_path = '/content/data/dogImages/test'\n",
        "\n",
        "batch_size=64\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        # '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        # '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "        test_path,\n",
        "        # '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6680 images belonging to 133 classes.\n",
            "Found 835 images belonging to 133 classes.\n",
            "Found 836 images belonging to 133 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SNaCdmyptuq"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3cqD79Mptuw",
        "outputId": "0fe5c842-a2ba-40d5-bcd6-080cdb897ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from time import time\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=( 150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "# model.add(Conv2D(64, (3, 3)))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(133))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-iPc-mOptu4"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nBBOZyDptu5"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awa4azQRptu8"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BKu8zYNptu-",
        "outputId": "bb2f6854-fb72-468a-f2f8-4ea663845683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        #steps_per_epoch=18631 // batch_size,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #validation_steps=10119 // batch_size\n",
        "        )\n",
        "model.save_weights('first_try.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "105/105 [==============================] - 92s 875ms/step - loss: 4.8920 - acc: 0.0086 - val_loss: 4.8711 - val_acc: 0.0156\n",
            "Epoch 2/16\n",
            "105/105 [==============================] - 91s 863ms/step - loss: 4.7967 - acc: 0.0196 - val_loss: 4.6907 - val_acc: 0.0299\n",
            "Epoch 3/16\n",
            "105/105 [==============================] - 91s 868ms/step - loss: 4.5571 - acc: 0.0423 - val_loss: 4.4803 - val_acc: 0.0455\n",
            "Epoch 4/16\n",
            "105/105 [==============================] - 91s 865ms/step - loss: 4.3324 - acc: 0.0559 - val_loss: 4.3741 - val_acc: 0.0527\n",
            "Epoch 5/16\n",
            "105/105 [==============================] - 91s 864ms/step - loss: 4.1674 - acc: 0.0750 - val_loss: 4.2057 - val_acc: 0.0671\n",
            "Epoch 6/16\n",
            "105/105 [==============================] - 90s 857ms/step - loss: 4.0481 - acc: 0.0872 - val_loss: 4.2626 - val_acc: 0.0743\n",
            "Epoch 7/16\n",
            "105/105 [==============================] - 91s 866ms/step - loss: 3.9246 - acc: 0.1028 - val_loss: 4.1273 - val_acc: 0.0766\n",
            "Epoch 8/16\n",
            "105/105 [==============================] - 90s 858ms/step - loss: 3.8086 - acc: 0.1184 - val_loss: 4.1311 - val_acc: 0.0802\n",
            "Epoch 9/16\n",
            "105/105 [==============================] - 91s 864ms/step - loss: 3.6883 - acc: 0.1422 - val_loss: 4.0051 - val_acc: 0.1066\n",
            "Epoch 10/16\n",
            "105/105 [==============================] - 91s 866ms/step - loss: 3.5760 - acc: 0.1595 - val_loss: 4.0250 - val_acc: 0.0958\n",
            "Epoch 11/16\n",
            "105/105 [==============================] - 91s 866ms/step - loss: 3.4707 - acc: 0.1769 - val_loss: 3.9706 - val_acc: 0.1042\n",
            "Epoch 12/16\n",
            "105/105 [==============================] - 91s 866ms/step - loss: 3.3777 - acc: 0.1957 - val_loss: 3.9863 - val_acc: 0.1006\n",
            "Epoch 13/16\n",
            "105/105 [==============================] - 91s 870ms/step - loss: 3.2663 - acc: 0.2151 - val_loss: 4.0878 - val_acc: 0.1054\n",
            "Epoch 14/16\n",
            "105/105 [==============================] - 91s 864ms/step - loss: 3.1679 - acc: 0.2354 - val_loss: 4.1432 - val_acc: 0.1030\n",
            "Epoch 15/16\n",
            "105/105 [==============================] - 91s 864ms/step - loss: 3.0796 - acc: 0.2467 - val_loss: 4.0802 - val_acc: 0.1114\n",
            "Epoch 16/16\n",
            "105/105 [==============================] - 91s 867ms/step - loss: 2.9792 - acc: 0.2694 - val_loss: 4.2457 - val_acc: 0.1150\n",
            "time taken  1454.0780708789825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUa_Q1fbptw3"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTnEXNMUFKXG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6236870b-f4b9-4e3f-b881-f561d8cea289"
      },
      "source": [
        "# test_prediction = model.predict(test_generator)\n",
        "model.metrics_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_7vgEAOCJ9_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "723796f1-a258-4f01-8b3c-800632d45782"
      },
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "# model.evaluate_generator(test_generator)\n",
        "test_generator.reset()\n",
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "test_eval = model.evaluate_generator(test_generator,STEP_SIZE_TEST)\n",
        "print('test loss ',test_eval[0])\n",
        "print('test accuracy ',test_eval[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss  4.124664765137893\n",
            "test accuracy  0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsjDgwHQptxC"
      },
      "source": [
        "---\n",
        "<a id='step4'></a>\n",
        "## Step 4: Create a CNN to Classify Dog Breeds (Optimization)\n",
        "\n",
        "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHCCoLBVptxD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "d167548c-99bf-4046-c0f3-6134877deae9"
      },
      "source": [
        "## TODO: Specify data loaders\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_path = '/content/data/dogImages/train/'\n",
        "val_path = '/content/data/dogImages/valid'\n",
        "test_path = '/content/data/dogImages/test'\n",
        "\n",
        "batch_size=64\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        # '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        # '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "        test_path,\n",
        "        # '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found 6680 images belonging to 133 classes.\n",
            "Found 835 images belonging to 133 classes.\n",
            "Found 836 images belonging to 133 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U_r6c3XptxF"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nmyc1viptxK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "2bbb8ac7-6190-4330-c632-730cd8ea1d15"
      },
      "source": [
        "from time import time\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "\n",
        "img_rows, img_cols, img_channel = 150, 150, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                     ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(133, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE-XFMlmptxN"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVXG7VDWptxO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "127e757d-0722-4625-c56e-7d96aad049d6"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQX2Ajmaptxm"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-QxP2c2ptxo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "90363eb4-f659-4b2b-a2f1-33ea7dc0ad23"
      },
      "source": [
        "check_point_name = 'vgg16.model'\n",
        "model_weights = 'vgg16.h5'\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=2,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint(check_point_name, monitor='val_acc', save_best_only=True)])\n",
        "model.save_weights(model_weights)\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/2\n",
            "105/105 [==============================] - 93s 890ms/step - loss: 0.6722 - acc: 0.5667 - val_loss: 0.6494 - val_acc: 0.6067\n",
            "Epoch 2/2\n",
            "105/105 [==============================] - 90s 853ms/step - loss: 0.6352 - acc: 0.6307 - val_loss: 0.6239 - val_acc: 0.6455\n",
            "time taken  183.86760330200195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfnQGw-kptxs"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n0TRq-Pptxs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a8cfc199-7d9e-4055-fc8d-ca6fcbc7f3fd"
      },
      "source": [
        "# test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)\n",
        "test_generator.reset()\n",
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "test_eval = model.evaluate_generator(test_generator,STEP_SIZE_TEST)\n",
        "print('test loss ',test_eval[0])\n",
        "print('test accuracy ',test_eval[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss  0.6086689554727994\n",
            "test accuracy  0.6575513298694904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bth7K3xCptxv"
      },
      "source": [
        "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc8xedlVptxx"
      },
      "source": [
        "### TODO: Write a function that takes a path to an image as input\n",
        "### and returns the dog breed that is predicted by the model.\n",
        "\n",
        "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
        "# class_names = [item[4:].replace(\"_\", \" \") for item in data_transfer['train'].classes]\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage import transform\n",
        "\n",
        "def load(filename):\n",
        "   np_image = Image.open(filename)\n",
        "   np_image = np.array(np_image).astype('float32')/255\n",
        "   np_image = transform.resize(np_image, (150, 150, 3))\n",
        "   np_image = np.expand_dims(np_image, axis=0)\n",
        "   return np_image\n",
        "\n",
        "def predict_breed_transfer(img_path):\n",
        "    # load the image and return the predicted breed\n",
        "  image = load(img_path)\n",
        "  y_prob = model.predict(image)\n",
        "  y_class = y_prob.argmax(axis=-1)\n",
        "  # print(y_classes)\n",
        "  labels = (train_generator.class_indices)\n",
        "  labels = dict((v,k) for k,v in labels.items())\n",
        "  prediction = [labels[k] for k in y_class]\n",
        "  return prediction    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5nZ73TxptyL"
      },
      "source": [
        "---\n",
        "<a id='step5'></a>\n",
        "## Step 5:  Algorithm\n",
        "\n",
        "\n",
        "### Implementation of Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBB5RFGXptyO"
      },
      "source": [
        "### TODO: Write your algorithm.\n",
        "### Feel free to use as many code cells as needed.\n",
        "\n",
        "def run_app(img_path):\n",
        "    ## handle cases for a human face, dog, and neither\n",
        "    if haar_face_detector(img_path):\n",
        "      print('hello human')\n",
        "      prediction = predict_breed_transfer(img_path)\n",
        "      print(prediction)\n",
        "    elif dog_detector(img_path):\n",
        "      print('hello dog')\n",
        "      prediction = predict_breed_transfer(img_path)\n",
        "      print(prediction)\n",
        "    else:\n",
        "      print(\"couldn't detect dog or human image\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxzD7XSpptyU"
      },
      "source": [
        "---\n",
        "<a id='step6'></a>\n",
        "## Step 6: Testing the Algorithm\n",
        "\n",
        "### (IMPLEMENTATION) Test the Algorithm on Sample Images!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO6WqBg9ZvxA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a097f44a-fc35-44d7-9817-b0bdac7d1593"
      },
      "source": [
        "dog_files[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['/content/data/dogImages/train/008.American_staffordshire_terrier/American_staffordshire_terrier_00547.jpg',\n",
              "       '/content/data/dogImages/train/008.American_staffordshire_terrier/American_staffordshire_terrier_00616.jpg',\n",
              "       '/content/data/dogImages/train/008.American_staffordshire_terrier/American_staffordshire_terrier_00612.jpg'],\n",
              "      dtype='<U113')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLJGHe8EptyY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "75d5f81e-a459-498d-89f1-f1820c260ed7"
      },
      "source": [
        "## TODO: Execute your algorithm from Step 6 on\n",
        "## at least 6 images on your computer.\n",
        "## Feel free to use as many code cells as needed.\n",
        "\n",
        "## suggested code, below\n",
        "for file in np.hstack((human_files[:3], dog_files[:3])):\n",
        "    run_app(file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello human\n",
            "['123.Pomeranian']\n",
            "hello human\n",
            "['013.Australian_terrier']\n",
            "hello human\n",
            "['123.Pomeranian']\n",
            "hello dog\n",
            "['123.Pomeranian']\n",
            "hello human\n",
            "['123.Pomeranian']\n",
            "hello dog\n",
            "['123.Pomeranian']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}